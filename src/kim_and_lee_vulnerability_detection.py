import os
import logging
import timeit
import json

import pdg_js.utility_df as utility_df

from extension_communication import build_extension_pdg
import utility
from pdg_js.node import Node, Identifier
from vulnerability_detection import store_analysis_results, default

PRINT_DEBUG = utility.PRINT_DEBUG


def analyze_extension(cs_path, bp_path, json_analysis=None, pdg=False, chrome=True, war=False,
                      json_messages=None, json_apis='permissions', manifest_path=None):
    res_dict = dict()
    extension_path = res_dict['extension'] = os.path.dirname(cs_path)
    benchmarks = res_dict['benchmarks'] = dict()
    messages_dict = dict()

    if manifest_path is None:
        manifest_path = os.path.join(extension_path, 'manifest.json')

    # cf. check_permissions.py:permission_check() and check_permissions.py:permission_check_v3():
    try:
        manifest = json.load(open(manifest_path))
        res_dict['manifest_version'] = manifest['manifest_version']
        urls = [cs['matches'] for cs in manifest['content_scripts']]
        res_dict['content_script_injected_into'] = [x for xs in urls for x in xs]  # flatten list of lists of URLs
    except FileNotFoundError:
        logging.critical('No manifest file found in %s', manifest_path)

    pdg_cs, pdg_bp = build_extension_pdg(cs_path=cs_path, bp_path=bp_path, benchmarks=benchmarks,
                                         pdg=pdg, chrome=chrome, messages_dict=messages_dict)

    logging.info('Finished to link CS with BP using the message passing APIs')

    no_added_df_edges_cs = add_missing_data_flow_edges(pdg_cs)
    print(f"{no_added_df_edges_cs} missing data flows edges added to CS PDG")
    no_added_df_edges_bp = add_missing_data_flow_edges(pdg_bp)
    print(f"{no_added_df_edges_bp} missing data flows edges added to BP PDG")

    if os.environ.get('PRINT_PDGS') == "yes":
        print()
        print(f"PDG (CS):\n{pdg_cs}")  # <pdg_js.node.Node object>
        print()
        print(f"PDG (BP):\n{pdg_bp}")
        print()

    res_dict["bp"] = dict()
    bp_exfiltration_dangers = res_dict["bp"]['exfiltration_dangers'] = []
    bp_infiltration_dangers = res_dict["bp"]['infiltration_dangers'] = []

    res_dict["cs"] = dict()
    cs_exfiltration_dangers = res_dict["cs"]['exfiltration_dangers'] = []
    cs_infiltration_dangers = res_dict["cs"]['infiltration_dangers'] = []

    try:
        with utility_df.Timeout(600):  # Tries to analyze an extension within 10 minutes
            detect_41_31_vuln_in_bp(pdg_bp=pdg_bp, results=bp_exfiltration_dangers, benchmarks=benchmarks, uxss=False)
            #detect_41_31_vuln_in_bp(pdg_bp=pdg_bp, res_dict=bp_infiltration_dict, uxss=True) # ToDo

    except utility_df.Timeout.Timeout:
        logging.exception('Analyzing the extension timed out for %s %s', cs_path, bp_path)
        if 'crashes' not in benchmarks:
            benchmarks['crashes'] = []
        benchmarks['crashes'].append('extension-analysis-timeout')

    if PRINT_DEBUG:
        print(json.dumps(res_dict, indent=4, sort_keys=False, default=default, skipkeys=True))
    else:
        store_analysis_results(extension_path, json_analysis, json_messages,
                               res_dict, messages_dict, outfile_name="analysis_renderer_attacker.json")


def get_all_identifiers(node):
    """
    Returns a list of all the Identifiers occurring in the given PDG subtree.
    """
    if isinstance(node, Identifier):
        return [node]
    else:
        result = []
        for child in node.children:
            result.extend(get_all_identifiers(child))
        return result


def add_missing_data_flow_edges(pdg):
    """
    Sadly, in the PDGs generated by DoubleX, some data flow edges that we need are missing.
    This function adds those missing data flow edges to the given PDG, e.g. those from y to x in a "x=y" assignment
    expression (including those cases where the "y" on the RHS is part of a more complex expression: "x=foo(y)").

    Parameters:
        pdg: the PDG (Program Dependence Graph), generated by DoubleX, to add the missing data flow edges to

    Returns:
        the number of data flow edges added to the given PDG, as an integer
    """

    # VariableDeclarator: let x=y, var x=y, const x=y

    # interface VariableDeclaration {
    #     declarations: VariableDeclarator[];
    #     kind: 'var' | 'const' | 'let';
    # }

    # interface VariableDeclarator {
    #     id: Identifier | BindingPattern;
    #     init: Expression | null;
    # }

    # type BindingPattern = ArrayPattern | ObjectPattern;

    # => Ignore VariableDeclarators like "let foo;" that have no right-hand-side!

    # AssignmentExpression: x=y

    # interface AssignmentExpression {
    #     operator: '=' | '*=' | '**=' | '/=' | '%=' | '+=' | '-=' | '<<=' | '>>=' | '>>>=' | '&=' | '^=' | '|=';
    #     left: Expression;
    #     right: Expression;
    # }

    # ==> Handle "Destructuring Assignments" like:
    #     "let [a, b] = [x, y];"             -> should create a flow from "x" to "a" and another one from "y" to "b"
    #     "const { a: a1, b: b1 } = obj;"    -> should create a flow from "obj" to "a1" and from "obj" to "b1"
    #       -> https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment

    # ==> Handle declarations/assignments to arbitrary expressions like:
    #     "let first_cookie = cookies[0];"     -> should create a flow from "cookies" to "first_cookie"
    #     "let first_cookie = cookies[1+2+3];" -> should create a flow from "cookies" to "first_cookie"
    #     "let x = y.z;"                       -> should create a flow from "y" to "x"
    #       -> https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Property_accessors
    #     "let first_cookie = JSON.stringify(cookies[0]);"    -> should create a flow from "cookies" to "first_cookie"

    # Example PDG subtrees:

    # ***** "let cookies2 = cookies;" or "var cookies2 = cookies;" or "const cookies2 = cookies;": *****
    # [1] [VariableDeclaration] (1 child)
    #     [2] [VariableDeclarator] (2 children)
    #         [3] [Identifier:"cookies2"] (0 children)
    #         [4] [Identifier:"cookies"] (0 children)

    # ***** "let cookies;": *****
    # [1] [VariableDeclaration] (1 child)
    #     [2] [VariableDeclarator] (1 child)
    #         [3] [Identifier:"cookies"] (0 children)

    # ***** "cookies2 = cookies;": *****
    # [1] [ExpressionStatement] (1 child)
    #     [2] [AssignmentExpression] (2 children)
    #         [3] [Identifier:"cookies2"] (0 children)
    #         [4] [Identifier:"cookies"] (0 children)

    # ***** "let [cookies2, forty_two] = [cookies, 42];": *****
    # [1] [VariableDeclaration] (1 child)
    #     [2] [VariableDeclarator] (2 children)
    #         [3] [ArrayPattern] (2 children)
    #             [4] [Identifier:"cookies2"] (0 children)
    #             [5] [Identifier:"forty_two"] (0 children)
    #         [6] [ArrayExpression] (2 children)
    #             [7] [Identifier:"cookies"] (0 children)
    #             [8] [Literal:"42"] (0 children)

    # Note that "let [four, five] = [4,5,6];" works as well.
    # "let [six, seven, eight] = [6,7];" works too, creating a variable "eight" that is undefined.
    # "[six, seven, eight] = [6,7];" sets the variable "eight" to undefined (or creating it).

    # ***** "const { a: cookies2, b: forty_two } = {a:cookies, b:42};": *****
    # [1] [VariableDeclaration] (1 child)
    #     [2] [VariableDeclarator] (2 children)
    #         [3] [ObjectPattern] (2 children)
    #             [4] [Property] (2 children)
    #                 [5] [Identifier:"a"] (0 children)
    #                 [6] [Identifier:"cookies2"] (0 children)
    #             [7] [Property] (2 children)
    #                 [8] [Identifier:"b"] (0 children)
    #                 [9] [Identifier:"forty_two"] (0 children)
    #         [10] [ObjectExpression] (2 children)
    #             [11] [Property] (2 children)
    #                 [12] [Identifier:"a"] (0 children)
    #                 [13] [Identifier:"cookies"] (0 children)
    #             [14] [Property] (2 children)
    #                 [15] [Identifier:"b"] (0 children)
    #                 [16] [Literal:"42"] (0 children)

    data_flow_edges_added = 0

    if (pdg.name == "VariableDeclarator" or pdg.name == "AssignmentExpression") and len(pdg.children) > 1:
        lhs = pdg.children[0]
        rhs = pdg.children[1]

        if isinstance(lhs, Identifier):  # "let cookies2 = cookies;" or "var cookies2 = cookies;" or "const cookies2 = cookies;" or "cookies2 = cookies;"
            for identifier in get_all_identifiers(rhs):  # For each identifier in the right-hand side...:
                # ...add a data flow edge *from* that identifier *to* the left-hand side:
                identifier.set_data_dependency(lhs)  # includes call: identifier.data_dep_children.append(extremity=lhs)
                data_flow_edges_added += 1

        elif lhs.name == "ArrayPattern" and rhs.name == "ArrayExpression":  # "let [cookies2, forty_two] = [cookies, 42];"
            for i in range(min(len(lhs.children), len(rhs.children))):
                for identifier in get_all_identifiers(rhs.children[i]):
                    identifier.set_data_dependency(lhs.children[i])
                    data_flow_edges_added += 1

        elif lhs.name == "ObjectPattern" and rhs.name == "ObjectExpression":  # "const { a: cookies2, b: forty_two } = {a:cookies, b:42};"
            # Note that, unlike in the example, the keys of the LHS and RHS might be in a different order!
            lhs_keys = [property_.children[0].attributes['name'] for property_ in lhs.children if property_.name == "Property" and isinstance(property_.children[0], Identifier)]
            rhs_keys = [property_.children[0].attributes['name'] for property_ in rhs.children if property_.name == "Property" and isinstance(property_.children[0], Identifier)]
            data_flow_keys = set.intersection(set(lhs_keys), set(rhs_keys))  # in the example: set("a", "b")
            for key in data_flow_keys:
                lhs_value = [property_.children[1] for property_ in lhs.children if property_.name == "Property" and len(property_.children) > 1 and isinstance(property_.children[0], Identifier) and property_.children[0].attributes['name'] == key][0]
                rhs_value = [property_.children[1] for property_ in rhs.children if property_.name == "Property" and len(property_.children) > 1 and isinstance(property_.children[0], Identifier) and property_.children[0].attributes['name'] == key][0]
                for identifier in get_all_identifiers(rhs_value):
                    identifier.set_data_dependency(lhs_value)
                    data_flow_edges_added += 1

        else:
            print(f"[Warning] Unknown type of {pdg.name} found in {pdg.get_file()}, line {pdg.get_line()} - possible missed data flow(s)")

    return data_flow_edges_added + sum(add_missing_data_flow_edges(child) for child in pdg.children)


def get_all_cookie_identifiers(pdg):
    """
    Returns all "cookie" (or arbitrarily named) identifiers coming from a
    ```
    chrome.cookies.getAll({}, function(cookies) { /* ... */ });
    ```
    or
    ```
    chrome.cookies.getAll({}, (cookies) => { /* ... */ });
    ```
    call.
    All these identifiers potentially contain very sensitive cookie data and shall not get into a
    content-script-accessible sink w/o proper authentication of the content script's URL!
    """
    pattern =\
        Node("CallExpression")\
            .child(
                Node("MemberExpression")
                    .child(
                        Node("MemberExpression")
                            .child(Node.identifier("chrome"))
                            .child(Node.identifier("cookies"))
                    )
                    .child(
                        Node.identifier("getAll")  # ToDo: handle chrome.cookies.get() as well!
                    )
            )

    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"Cookie Pattern #1:\n{pattern}")

    result = []
    for pattern_match in pdg.find_pattern(pattern,
                                          match_identifier_names=True,
                                          match_literals=False,
                                          allow_additional_children=True):
        if pattern_match.has_child("FunctionExpression"):  # case 1: FunctionExpression:
            result.append(pattern_match.get_child("FunctionExpression").get_child("Identifier"))
        elif pattern_match.has_child("ArrowFunctionExpression"):  # case 2: ArrowFunctionExpression:
            result.append(pattern_match.get_child("ArrowFunctionExpression").get_child("Identifier"))
    return result


def get_all_sendResponse_sinks(pdg):
    """
    Returns all "sendResponse" (or arbitrarily named) identifiers coming from a
    ```
    chrome.runtime.onMessage.addListener(function (message, sender, sendResponse) { /* ... * });
    ```
    or
    ```
    chrome.runtime.onMessage.addListener((message, sender, sendResponse) => { /* ... * });
    ```
    call.
    All these identifiers describe content-script-accessible sinks!
    """
    pattern =\
        Node("CallExpression")\
            .child(
                Node("MemberExpression")
                    .child(
                        Node("MemberExpression")
                            .child(
                                Node("MemberExpression")
                                    .child(Node.identifier("chrome"))   # Note that the "chrome" is necessary, simply
                                    .child(Node.identifier("runtime"))  # calling "runtime.onMessage" doesn't work!
                            )
                            .child(Node.identifier("onMessage"))
                    )
                    .child(
                        Node.identifier("addListener")
                    )
            )
            #.child(
            #    Node("FunctionExpression") or Node("ArrowFunctionExpression")
            #)

    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"SendResponse Pattern #1:\n{pattern}")

    result = []
    for pattern_match in pdg.find_pattern(pattern,
                                          match_identifier_names=True,
                                          match_literals=False,
                                          allow_additional_children=True):

        if pattern_match.has_child("FunctionExpression"):  # case 1: FunctionExpression:
            if len(pattern_match.get_child("FunctionExpression").children) >= 4:  # (msg, sndr, sendResponse, block)
                result.append(pattern_match.get_child("FunctionExpression").children[2])  # (msg, sndr, *sendResponse*)

        elif pattern_match.has_child("ArrowFunctionExpression"):  # case 2: ArrowFunctionExpression:
            if len(pattern_match.get_child("ArrowFunctionExpression").children) >= 4:  # (msg, sndr, sendResponse, block)
                result.append(pattern_match.get_child("ArrowFunctionExpression").children[2])  # (msg, sndr, *sendResponse*)

        # Note that instead of (message, sender, sendResponse), the programmer may leave out trailing parameters:
        # either (message, sender) or just (message).
        # In that case however, there exists no `sendResponse` sink which this function could return.
        # When the programmer added additional redundant parameters though, e.g., (msg, sndr, sendResponse, foo, block),
        # this doesn't matter.
        # Therefore, we check if the (Arrow)FunctionExpression has *at least* 4 children.
    return result


class DataFlow:
    def __init__(self, nodes):
        self.nodes = nodes

    def pretty(self):  # => result is used later by json.dump()
        return [{
            "no": i+1,
            "location": self.nodes[i].get_location(),
            "filename": self.nodes[i].get_file(),
            "identifier": self.nodes[i].attributes['name'],
            "line_of_code": self.nodes[i].get_whole_line_of_code_as_string()
        } for i in range(len(self.nodes))]

    @classmethod
    def from_node_list(cls, node_list):
        return DataFlow(node_list)

    @classmethod
    def beginning_at(cls, initial_node):
        return DataFlow([initial_node])

    def may_continue(self):
        """
        Returns True iff this data flow may be continued.
        """
        return len(self.nodes[-1].data_dep_children) > 0

    def continue_flow(self):
        """
        Returns a list of all possible (1-step) continuations of this DataFlow (being DataFlows themselves),
        or `None` if this DataFlow cannot be continued any further.
        """
        result = []
        next = self.nodes[-1].data_dep_children
        if len(next) == 0:
            return None  # indicate that this DataFlow cannot be continued any further
        for child_data_dep in next:
            result.append(DataFlow.from_node_list(self.nodes + [child_data_dep.extremity]))
        return result

    def get_all_continued_flows(self):
        """
        Returns a list of all possible (n-step) continuations of this DataFlow (being DataFlows themselves).
        Because of repeated branching, the list returned can, in theory, be arbitrarily long.
        Returns `[self]` if this DataFlow cannot be continued any further.
        """
        data_flows = [self]  # may remain a list of 1 item if there's just 1 flow, may split up
        while any(df.may_continue() for df in data_flows):
            for idx in [i for i in range(len(data_flows)) if data_flows[i].may_continue()]:
                # Continue data flow, might result in a split into multiple new data flows:
                continued_flow = data_flows[idx].continue_flow()  # (not None due to check above)
                data_flows = data_flows[:idx] + continued_flow + data_flows[idx + 1:]
        return data_flows

    def last_node(self):
        return self.nodes[-1]


def data_flows_into_function(pdg, from_node, to_node, return_multiple=False):
    """
    Returns the empty list `[]` if no data flow exists from `from_node` to `to_node`.
    Otherwise, returns the data flow(s) from `from_node` to `to_node` as a (JSON-dump-able) dictionary.
    The data flow is returned as a "from_flow" (from `from_node` to a "CallExpression")
    and a "to_flow" (from `to_node` to the *same* "CallExpression").

    Parameters:
        pdg: the entire PDG
        from_node: an Identifier Node of a value containing sensitive data (e.g., cookies)
        to_node: an Identifier Node of a function representing a dangerous sink (e.g., sendResponse)
        return_multiple: a boolean indicating whether to return multiple data flows if multiple data flows exist from
                         the given `from_node` to the given `to_node`; when set to False, the returned list will either
                         be empty or contain exactly 1 element; when set to True, the returned list can have arbitrary
                         length

    Returns:
        Returns the empty list `[]` if no data flow exists from `from_node` to `to_node`.
        Otherwise, returns the data flow(s) from `from_node` to `to_node` as a (JSON-dump-able) dictionary.
        The data flow is returned as a "from_flow" (from `from_node` to a "CallExpression")
        and a "to_flow" (from `to_node` to the *same* "CallExpression").
    """
    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"Looking for data flow in PDG [{pdg.id}] from node [{from_node.id}] to function [{to_node.id}] ...")

    data_flows_from = DataFlow.beginning_at(from_node).get_all_continued_flows()

    data_flows_to = DataFlow.beginning_at(to_node).get_all_continued_flows()

    # Check if any data flow from `data_flows_from` and any data flow from `data_flows_to` end up in the same
    #   CallExpression; if so return both of these data flows:
    results = []
    for from_flow in data_flows_from:
        for to_flow in data_flows_to:
            if from_flow.last_node().parent.id == to_flow.last_node().parent.id and from_flow.last_node().parent.name == "CallExpression":
                result = {
                    "from_flow": from_flow.pretty(),
                    "to_flow": to_flow.pretty(),
                    "function_call": {
                        "location": from_flow.last_node().parent.get_location(),
                        "filename": from_flow.last_node().parent.get_file(),
                        "line_of_code": from_flow.last_node().parent.get_whole_line_of_code_as_string()
                    }
                }
                if return_multiple:
                    result["variant"] = len(results) + 1  # count/number the data flows when return_multiple=True
                    results.append(result)
                else:
                    return [result]
    return results


def detect_41_31_vuln_in_bp(pdg_bp, results, benchmarks, uxss=False):  # ToDo: handle uxss=True  # ToDo: check authentication
    """
    Look for type 4.1 vulnerabilities in the given background page/service worker (or rather its PDG), type 4.1
    vulnerabilities refer to the ability to `Execute Privileged Browser APIs` w/o (sufficiently) verifying `sender.url`,
    which is a security violation of type 3.1: `Extension Message Authentication`
    (refer to Kim and Lee paper for more info)

    Example vulnerability (with no authentication of `sender.url` whatsoever):
    ```
    chrome.runtime.onMessage.addListener((msg, sender, sendResponse) => {
        chrome.cookies.getAll({},
            function(cookies) {
                sendResponse(cookies);
            }
        );
        return true;
    });
    ```
    """
    start = timeit.default_timer()

    cookies_sources = get_all_cookie_identifiers(pdg_bp)
    sendResponse_sinks = get_all_sendResponse_sinks(pdg_bp)

    if os.environ.get('PRINT_PDGS') == "yes":
        for cookies_source in cookies_sources:
            print(f"cookies source: {cookies_source}")
        for sendResponse_sink in sendResponse_sinks:
            print(f"sendResponse sink: {sendResponse_sink}")

    for cookies_source in cookies_sources:
        for sendResponse_sink in sendResponse_sinks:
            data_flows = data_flows_into_function(pdg_bp,
                                                  cookies_source,
                                                  sendResponse_sink,
                                                  return_multiple=(os.environ.get('RETURN_MULTIPLE_FLOW_VARIANTS') == "yes"))
            for data_flow in data_flows:
                print(f"[4.1/3.1] Data flow found:\n{json.dumps(data_flow, indent=4, sort_keys=False,skipkeys=True)}\n")
                results.append(data_flow)

    time_diff = timeit.default_timer() - start
    print(f'Successfully analyzed BP for 4.1/3.1 vulnerabilities in {time_diff}s')
    benchmarks[f"bp: 4.1/3.1 vulnerabilities{' (UXSS)' if uxss else ''}"] = time_diff
