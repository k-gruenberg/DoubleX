import os
import logging
import timeit
import json

import pdg_js.utility_df as utility_df

from extension_communication import build_extension_pdg
import utility
from pdg_js.node import Node
from vulnerability_detection import store_analysis_results, default

PRINT_DEBUG = utility.PRINT_DEBUG


def analyze_extension(cs_path, bp_path, json_analysis=None, pdg=False, chrome=True, war=False,
                      json_messages=None, json_apis='permissions', manifest_path=None):
    res_dict = dict()
    extension_path = res_dict['extension'] = os.path.dirname(cs_path)
    benchmarks = res_dict['benchmarks'] = dict()
    messages_dict = dict()

    if manifest_path is None:
        manifest_path = os.path.join(extension_path, 'manifest.json')

    # cf. check_permissions.py:permission_check() and check_permissions.py:permission_check_v3():
    try:
        manifest = json.load(open(manifest_path))
        res_dict['manifest_version'] = manifest['manifest_version']
        urls = [cs['matches'] for cs in manifest['content_scripts']]
        res_dict['content_script_injected_into'] = [x for xs in urls for x in xs]  # flatten list of lists of URLs
    except FileNotFoundError:
        logging.critical('No manifest file found in %s', manifest_path)

    pdg_cs, pdg_bp = build_extension_pdg(cs_path=cs_path, bp_path=bp_path, benchmarks=benchmarks,
                                         pdg=pdg, chrome=chrome, messages_dict=messages_dict)

    logging.info('Finished to link CS with BP using the message passing APIs')

    if os.environ.get('PRINT_PDGS') == "yes":
        print()
        print(f"PDG (CS):\n{pdg_cs}")  # <pdg_js.node.Node object>
        print()
        print(f"PDG (BP):\n{pdg_bp}")
        print()

    res_dict["bp"] = dict()
    bp_exfiltration_dangers = res_dict["bp"]['exfiltration_dangers'] = []
    bp_infiltration_dangers = res_dict["bp"]['infiltration_dangers'] = []

    res_dict["cs"] = dict()
    cs_exfiltration_dangers = res_dict["cs"]['exfiltration_dangers'] = []
    cs_infiltration_dangers = res_dict["cs"]['infiltration_dangers'] = []

    try:
        with utility_df.Timeout(600):  # Tries to analyze an extension within 10 minutes
            detect_41_31_vuln_in_bp(pdg_bp=pdg_bp, results=bp_exfiltration_dangers, benchmarks=benchmarks, uxss=False)
            #detect_41_31_vuln_in_bp(pdg_bp=pdg_bp, res_dict=bp_infiltration_dict, uxss=True) # ToDo

    except utility_df.Timeout.Timeout:
        logging.exception('Analyzing the extension timed out for %s %s', cs_path, bp_path)
        if 'crashes' not in benchmarks:
            benchmarks['crashes'] = []
        benchmarks['crashes'].append('extension-analysis-timeout')

    if PRINT_DEBUG:
        print(json.dumps(res_dict, indent=4, sort_keys=False, default=default, skipkeys=True))
    else:
        store_analysis_results(extension_path, json_analysis, json_messages,
                               res_dict, messages_dict, outfile_name="analysis_renderer_attacker.json")


def get_all_cookie_identifiers(pdg):
    """
    Returns all "cookie" (or arbitrarily named) identifiers coming from a
    ```
    chrome.cookies.getAll({}, function(cookies) { /* ... */ });
    ```
    call.
    All these identifiers potentially contain very sensitive cookie data and shall not get into a
    content-script-accessible sink w/o proper authentication of the content script's URL!
    """
    pattern =\
        Node("CallExpression")\
            .child(
                Node("MemberExpression")
                    .child(
                        Node("MemberExpression")
                            .child(Node.identifier("chrome"))
                            .child(Node.identifier("cookies"))
                    )
                    .child(
                        Node.identifier("getAll")
                    )
            )

    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"Cookie Pattern #1:\n{pattern}")

    result = []
    for pattern_match in pdg.find_pattern(pattern,
                                          match_identifier_names=True,
                                          match_literals=False,
                                          allow_additional_children=True):
        result.append(pattern_match.get_child("FunctionExpression").get_child("Identifier"))
    return result


def get_all_sendResponse_sinks(pdg):
    """
    Returns all "sendResponse" (or arbitrarily named) identifiers coming from a
    ```
    chrome.runtime.onMessage.addListener((message, sender, sendResponse) => { /* ... * });
    ```
    call.
    All these identifiers describe content-script-accessible sinks!
    """
    pattern =\
        Node("CallExpression")\
            .child(
                Node("MemberExpression")
                    .child(
                        Node("MemberExpression")
                            .child(
                                Node("MemberExpression")
                                    .child(Node.identifier("chrome"))
                                    .child(Node.identifier("runtime"))
                            )
                            .child(Node.identifier("onMessage"))
                    )
                    .child(
                        Node.identifier("addListener")
                    )
            )\
            .child(
                Node("ArrowFunctionExpression")
            )

    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"SendResponse Pattern #1:\n{pattern}")

    result = []
    for pattern_match in pdg.find_pattern(pattern,
                                          match_identifier_names=True,
                                          match_literals=False,
                                          allow_additional_children=True):
        result.append(pattern_match.get_child("ArrowFunctionExpression").children[2])  # (msg, sndr, sendResponse)
    return result


class DataFlow:
    def __init__(self, nodes):
        self.nodes = nodes

    def pretty(self):  # => result is used later by json.dump()
        return [{
            "no": i+1,
            "location": self.nodes[i].get_location(),
            "filename": self.nodes[i].get_file(),
            "identifier": self.nodes[i].attributes['name'],
            "line_of_code": self.nodes[i].get_whole_line_of_code_as_string()
        } for i in range(len(self.nodes))]

    @classmethod
    def from_node_list(cls, node_list):
        return DataFlow(node_list)

    @classmethod
    def beginning_at(cls, initial_node):
        return DataFlow([initial_node])

    def may_continue(self):
        """
        Returns True iff this data flow may be continued.
        """
        return len(self.nodes[-1].data_dep_children) > 0

    def continue_flow(self):
        """
        Returns a list of all possible (1-step) continuations of this DataFlow (being DataFlows themselves),
        or `None` if this DataFlow cannot be continued any further.
        """
        result = []
        next = self.nodes[-1].data_dep_children
        if len(next) == 0:
            return None  # indicate that this DataFlow cannot be continued any further
        for child_data_dep in next:
            result.append(DataFlow.from_node_list(self.nodes + [child_data_dep.extremity]))
        return result

    def get_all_continued_flows(self):
        """
        Returns a list of all possible (n-step) continuations of this DataFlow (being DataFlows themselves).
        Because of repeated branching, the list returned can, in theory, be arbitrarily long.
        Returns `[self]` if this DataFlow cannot be continued any further.
        """
        data_flows = [self]  # may remain a list of 1 item if there's just 1 flow, may split up
        while any(df.may_continue() for df in data_flows):
            for idx in [i for i in range(len(data_flows)) if data_flows[i].may_continue()]:
                # Continue data flow, might result in a split into multiple new data flows:
                continued_flow = data_flows[idx].continue_flow()  # (not None due to check above)
                data_flows = data_flows[:idx] + continued_flow + data_flows[idx + 1:]
        return data_flows

    def last_node(self):
        return self.nodes[-1]


def data_flow_into_function(pdg, from_node, to_node):
    """
    Returns `None` if no data flow exists from `from_node` to `to_node`.
    Otherwise, returns the data flow from `from_node` to `to_node` as a (JSON-dump-able) dictionary.
    The data flow is returned as a "from_flow" (from `from_node` to a "CallExpression")
    and a "to_flow" (from `to_node` to the *same* "CallExpression").

    Parameters:
        pdg: the entire PDG
        from_node: an Identifier Node of a value containing sensitive data (e.g., cookies)
        to_node: an Identifier Node of a function representing a dangerous sink (e.g., sendResponse)

    Returns:
        Returns `None` if no data flow exists from `from_node` to `to_node`.
        Otherwise, returns the data flow from `from_node` to `to_node` as a (JSON-dump-able) dictionary.
        The data flow is returned as a "from_flow" (from `from_node` to a "CallExpression")
        and a "to_flow" (from `to_node` to the *same* "CallExpression").
    """
    if os.environ.get('PRINT_PDGS') == "yes":
        print(f"Looking for data flow in PDG [{pdg.id}] from node [{from_node.id}] to function [{to_node.id}] ...")

    data_flows_from = DataFlow.beginning_at(from_node).get_all_continued_flows()

    data_flows_to = DataFlow.beginning_at(to_node).get_all_continued_flows()

    # Check if any data flow from `data_flows_from` and any data flow from `data_flows_to` end up in the same
    #   CallExpression; if so return both of these data flows:
    for from_flow in data_flows_from:
        for to_flow in data_flows_to:
            if from_flow.last_node().parent.id == to_flow.last_node().parent.id and from_flow.last_node().parent.name == "CallExpression":
                result = {
                    "from_flow": from_flow.pretty(),
                    "to_flow": to_flow.pretty(),
                    "function_call": {
                        "location": from_flow.last_node().parent.get_location(),
                        "filename": from_flow.last_node().parent.get_file(),
                        "line_of_code": from_flow.last_node().parent.get_whole_line_of_code_as_string()
                    }
                }
                return result
    return None


def detect_41_31_vuln_in_bp(pdg_bp, results, benchmarks, uxss=False):  # ToDo: handle uxss=True  # ToDo: check authentication
    """
    Look for type 4.1 vulnerabilities in the given background page/service worker (or rather its PDG), type 4.1
    vulnerabilities refer to the ability to `Execute Privileged Browser APIs` w/o (sufficiently) verifying `sender.url`,
    which is a security violation of type 3.1: `Extension Message Authentication`
    (refer to Kim and Lee paper for more info)

    Example vulnerability (with no authentication of `sender.url` whatsoever):
    ```
    chrome.runtime.onMessage.addListener((msg, sender, sendResponse) => {
        chrome.cookies.getAll({},
            function(cookies) {
                sendResponse(cookies);
            }
        );
        return true;
    });
    ```
    """
    start = timeit.default_timer()

    cookies_sources = get_all_cookie_identifiers(pdg_bp)
    sendResponse_sinks = get_all_sendResponse_sinks(pdg_bp)

    if os.environ.get('PRINT_PDGS') == "yes":
        for cookies_source in cookies_sources:
            print(f"cookies source: {cookies_source}")
        for sendResponse_sink in sendResponse_sinks:
            print(f"sendResponse sink: {sendResponse_sink}")

    for cookies_source in cookies_sources:
        for sendResponse_sink in sendResponse_sinks:
            data_flow = data_flow_into_function(pdg_bp, cookies_source, sendResponse_sink)
            if data_flow is not None:
                print(f"[4.1/3.1] Data flow found:\n{json.dumps(data_flow, indent=4, sort_keys=False,skipkeys=True)}\n")
                results.append(data_flow)

    time_diff = timeit.default_timer() - start
    print(f'Successfully analyzed BP for 4.1/3.1 vulnerabilities in {time_diff}s')
    benchmarks[f"bp: 4.1/3.1 vulnerabilities{' (UXSS)' if uxss else ''}"] = time_diff
